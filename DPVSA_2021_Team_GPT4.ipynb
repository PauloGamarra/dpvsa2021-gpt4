{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPVSA 2021 - Team GPT4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRuiVvVj2hpK"
      },
      "source": [
        "# Computer Vision Challenge: Soccer Match Monitoring‚Äù at the IEEE DPVSA 2021 - Team GPT4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p9wjYH52r9p"
      },
      "source": [
        "## About\n",
        "\n",
        "Implementation of a soccer match monitoring system for the ‚ÄúComputer Vision Challenge: Soccer Match Monitoring‚Äù at the IEEE DPVSA 2021. The team is formed by CS undergraduate students at Federal University Of Rio Grande Do Sul (UFRGS), Paulo Gamarra Lessa Pinto, Thiago Sotoriva Lermen and Gabriel Couto Domingues mentored by the Professor Claudio Rosito Jung, PhD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2-XsaZC2vHI"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To run the code, first we need to clone the base repository from Github and install basic dependencies. After that we check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnTgY2ky2cVO",
        "outputId": "915a77c5-b784-486f-a284-b98217a9edb6"
      },
      "source": [
        "# Current directory\n",
        "print(f\"Current directory:\")\n",
        "!ls\n",
        "\n",
        "# Clone the github repository\n",
        "!git clone https://github.com/PauloGamarra/dpvsa2021-gpt4 \n",
        "%cd dpvsa2021-gpt4\n",
        "\n",
        "# Clone the Yolov5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%pip install -qr ./yolov5/requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "# Setup CUDA\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.9.0+cu111 (Tesla K80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PeJH8323tB8"
      },
      "source": [
        "To run the code with the test video, we recommend mount colab to import the data and run it with the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znv5cbYJ3Re0",
        "outputId": "ff282ece-7083-453a-91c2-4fb053074ad1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') # path to google colab"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTpHyzYx4CWg"
      },
      "source": [
        "Copy the video from your Gdrive to the local Colab's environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CN_aDIH4B18"
      },
      "source": [
        "!cp '/content/gdrive/My Drive/dpvsa2021-gpt4/<your_video_file_name> ./ # change this line to choose your input video"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txuDy6us4Y_R"
      },
      "source": [
        "## Run the model\n",
        "\n",
        "With all the setup done, we can now execute the object detection task with the following command:\n",
        "\n",
        "```\n",
        "!python detect_video.py --source <src_video_path>\n",
        "                        --output_video <output_video_path>\n",
        "                        --output_bboxes <results_path>\n",
        "                        --yolo_repo <yolo_repository_path>\n",
        "                        --model_weights <loaded_weights_path>\n",
        "                        --imsz <img_size>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPF1fbYu4b0f",
        "outputId": "1e8f4b7d-aa66-4b75-d242-cec8fa421fa4"
      },
      "source": [
        "# Pre defined command (edit the source argument)\n",
        "!python detect_video.py --source <your_video_file_name> --model_weights ./weights/dpvsa_detector_1080.pt --imsz 1080"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading video input from dpvsa_video1_mini.mp4\n",
            "writing video output to output_video.mp4\n",
            "loading model weights from ./weights/dpvsa_detector_1080.pt\n",
            "YOLOv5 üöÄ v6.0-35-ga4fece8 torch 1.9.0+cu111 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "using 1080 as model image input size\n",
            "Processing frames...\n",
            "100% 3658/3658 [08:50<00:00,  6.90it/s]\n",
            "Done! Output video saved to output_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKlpXuXQ6o5H"
      },
      "source": [
        "Copy the output video to the given path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bwbtd7D6msb"
      },
      "source": [
        "cp output_video.mp4 '/content/gdrive/My Drive/dpvsa2021-gpt4/output_video.mp4' # change this line to choose your output video path"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQoyFST26vbX"
      },
      "source": [
        "## Output data\n",
        "\n",
        "To validade the model we save the result for each frame as a list of bounding box arrays with the format `[(x1, y1, x2, y2, conf, class), ...]` as specified in the challange notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAqxVzsg67i3"
      },
      "source": [
        "import numpy as np\n",
        "result_path = '<path_to_result_npy_file>' # change this line to choose your results directory\n",
        "bbox_result = np.load(result_path)\n",
        "bbox_result = [tuple(bbox) for bbox in bbox_result]"
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}